<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
<title>Cirilla - LLM Project</title>
<link href="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/themes/prism-tomorrow.css" rel="stylesheet" />
<style>
    body {
        font-family: 'Inter', -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen,
                    Ubuntu, Cantarell, "Open Sans", "Helvetica Neue", sans-serif;
        background-color: #0b0b0b;
        color: #bbbbbb;
        line-height: 1.6;
        margin: 0;
        padding: 0;
        display: flex;
    }


    header {
        position: fixed;
        top: 0;
        left: 0;
        width: 100%;
        height: 60px;
        background: #111;
        display: flex;
        align-items: center;
        justify-content: space-between;
        padding: 0 20px;
        border-bottom: 1px solid #333;
        z-index: 1000;
    }

    header h1 {
        font-size: 1.2rem;
        margin: 0;
        margin-left: 300px;
    }

    .search-bar {
        background: #1a1a1a;
        border: 1px solid #333;
        border-radius: 8px;
        padding: 6px 10px;
        color: #fff;
        width: 250px;
        margin-right: 50px; /* move it left from the far edge */
        font-size: 1rem;
    }

    .sidebar h2 {
        font-size: 1rem;
        margin-bottom: 15px;
        color: #facc15;
    }

    .sidebar {
        position: fixed;
        top: 60px;
        left: 0;
        width: 200px;           /* make it wider if needed */
        height: 100%;
        background: #0b0b0b;
        border-right: 1px solid #333; 
        padding: 20px 20px 20px 20px; /* move content away from the line */
    }

    .sidebar a:hover {
        text-decoration: underline;
    }

    main.container {
        max-width: 1000px;
        margin: 80px auto;
        padding: 0 20px;
    }

    img {
        max-width: 100%;
        border-radius: 10px;
        display: block;
        margin: 20px auto;
    }

    .header-image {
        border-radius: 0;
        width: 100%;
        margin: 0;
    }

    .caption {
        font-size: 0.9em;
        color: gray;
        text-align: right;
        margin-bottom: 20px;
    }

    h1, h2, h3 {
        text-align: left;
        font-size: 1.5em;
        font-weight: 600;
        scroll-margin-top: 80px; /* offset for fixed header */
    }

    .flex-container {
        display: flex;
        flex-wrap: wrap;
        gap: 20px;
        margin-bottom: 40px;
    }

    .flex-container > div {
        flex: 1;
        min-width: 250px;
    }

    .text {
        padding: 10px;
    }

    .img-container {
        text-align: center;
    }

    .img-caption {
        font-size: 0.9em;
        color: gray;
        margin-top: 5px;
    }

    pre {
        background-color: #1a1a1a;
        padding: 15px;
        border-radius: 10px;
        overflow-x: auto;
    }

    .docs-section {
        margin-top: 60px;
        padding-top: 30px;
        border-top: 1px solid #333;
    }

    .function-doc {
        margin-bottom: 50px;
    }

    .function-signature {
        font-family: monospace;
        font-size: 1.1em;
        color: #4ade80;
        margin-bottom: 10px;
        margin-left: 5px;
    }

    .code-label {
        font-family: monospace;
        font-style: italic;
        font-size: 1.3em;
        color: #6fc28d;
        font-weight: 500;
        margin-bottom: 10px;
        margin-left: 5px;
    }

    .indented {
    margin-left: 35px;
    }

    table {
        border-collapse: collapse;
        width: 100%;
        margin: 15px 0;
    }

    table, th, td {
        border: 1px solid #333;
    }

    th, td {
        padding: 10px;
        text-align: left;
    }

    th {
        background-color: #1a1a1a;
        color: #d4ae13;
    }

    code {
        background: #222;
        font-weight: 100;
        padding: 2px 6px;
        border-radius: 4px;
        color: #96e2dc;
        font-size: 1.2em;
    }

    hr.doc-divider {
        border: none;
        border-top: 1px solid #333;
        margin: 40px 0;
    }

    p + pre {
        margin-top: 0;
    }

    .function-doc pre {
        margin-top: 0;
        margin-bottom: 15px; /* keep some spacing below */
    }

    pre {
        margin-top: 0 !important;      /* remove top space */
        margin-bottom: 15px;           /* optional spacing below */
        padding: 15px;
        border-radius: 10px;
        overflow-x: auto;
    }

    pre code {
        display: block;                /* ensures the code fills the pre */
        margin: 0;                     /* remove any extra margin */
        line-height: 1.4;              /* adjust if needed */
    }

    a[href="#rm_duplicate_instructs"] {
    color: #00698f;
    }

    a {
    color: #00698f;
    }


</style>
</head>
<body>

<header>
    <h1>Cirilla</h1>
    <input type="text" id="search" class="search-bar" placeholder="Search the docs ..." />
</header>

<!-- Sidebar with collapsible subcategories showing full names -->
<div class="sidebar" id="sidebar">

    <div class="category">
        <div class="category-header" data-toggle="model">
            <span class="header-text">cirilla.synth_data</span>
            <span class="arrow">▾</span>
        </div>
        <div class="category-items" id="model-items">
            <a href="#OllamaCurate">OllamaCurate</a>
            <a href="#rm_duplicate_instructs">rm_duplicate_instructs</a>
            <a href="#get_synth_reasoning_dataset">get_synth_reasoning_dataset</a>
            <a href="#vllm_multi_turn">vllm_multi_turn</a>
            <a href="#multi_turn_gather">multi_turn_gather</a>
            <a href="#gather_summaries">gather_summaries</a>
            <a href="#summaries_to_instruct">summaries_to_instruct</a>
        </div>
    </div>

    <div class="category">
        <div class="category-header" data-toggle="modules">
            <span class="header-text">fandom_scraper</span>
            <span class="arrow">▾</span>
        </div>
        <div class="category-items" id="modules-items">
            <a href="#scrape_fandom">scrape_fandom</a>
            <a href="#instructions_into_conv">instructions_into_conv</a>
        </div>
    </div>
</div>

<style>
.category-header {
    display: flex;
    justify-content: space-between; /* text left, arrow right */
    align-items: center;
    cursor: pointer;
    margin: 8px 0;
    color: #aaaaaa;
    font-size: 0.95rem;
    font-weight: normal;
}

.category-header .arrow {
    flex: 0 0 auto; /* arrow stays flush right */
}

.category-header .header-text {
    flex: 1;
    text-align: left; /* text stays flush left */
    transition: box-shadow 0.2s;
}

/* underline on hover without moving the text */
.category-header .header-text:hover {
    box-shadow: inset 0 -2px 0 0 #bbbbbb; 
}

.category-items {
    margin-left: 10px;
    display: none;
    flex-direction: column;
}

.category-items a {
    display: block;
    margin: 4px 0;
    color: #8bd1cb;
    text-decoration: none;
    font-size: 0.9rem;
}

.category-items a:hover {
    text-decoration: underline;
}
</style>

<script>
// Search functionality for sidebar links and function docs
const searchInput = document.getElementById('search');
searchInput.addEventListener('input', function() {
    const query = this.value.toLowerCase();

    // Sidebar links visibility
    const links = document.querySelectorAll('.sidebar a');
    links.forEach(link => {
        const text = link.textContent.toLowerCase();
        link.style.display = text.includes(query) ? 'block' : 'none';
    });

    // Function docs visibility
    const functions = document.querySelectorAll('.function-doc');
    let firstMatch = null;
    functions.forEach(fn => {
        const text = fn.innerText.toLowerCase();
        if (text.includes(query)) {
            fn.style.display = '';
            if (!firstMatch) firstMatch = fn;
        } else {
            fn.style.display = 'none';
        }
    });

    // Scroll to first matching function doc or to Documentation header if query exists
    const docsHeader = document.querySelector('.docs-section h2');
    if (query && docsHeader) {
        const yOffset = -80; // offset for fixed header
        const y = (firstMatch ? firstMatch : docsHeader).getBoundingClientRect().top + window.pageYOffset + yOffset;
        window.scrollTo({ top: y, behavior: 'smooth' });
    }
});
</script>



<main class="container">

    <img src="https://github.com/AnthonyP57/Radovid---a-LLM-made-on-a-budget/blob/master/img/ciri_w4_2.png?raw=true" class="header-image" alt="Ciri from The Witcher 4 trailer">
    <div class="caption"><em>Ciri from The Witcher 4 trailer</em></div>

    <!-- <h1>Cirilla</h1> -->
    <p>Cirilla is an open source learning project aiming at implementing various LLMs. It is focused mainly on showing how to make, train, infer and deploy a LLM from scratch using PyTorch and a budget-friendly GPU (RTX 4060Ti 16GiB ~500$).</p>

    <h2>Who is Cirilla</h2>
    <div class="flex-container">
        <div class="img-container">
            <img src="https://github.com/AnthonyP57/Radovid---a-LLM-made-on-a-budget/blob/master/img/fake_ciri.webp?raw=true" alt="Ciri" height="400">
            <div class="img-caption"><em>Fig.1 Ciri Gwent card by Bogna Gawrońska</em></div>
        </div>
        <div class="text">
            <p><strong>Cirilla Fiona Elen Riannon</strong>, known as <em>Ciri</em>, is one of the central characters in <em>The Witcher</em> saga by Andrzej Sapkowski and its adaptations. She is the princess of Cintra, granddaughter of Queen Calanthe, and the sole heir to a powerful lineage marked by the mysterious Elder Blood.</p>
            <p>Ciri is defined by her destiny, adaptability, and potential. Unlike kings who wield authority by birthright, her strength comes from surviving chaos, learning from mentors like Geralt and Yennefer, and unlocking extraordinary powers.</p>
            <p>Her unique abilities make her one of the most pivotal figures in the saga. Known as the <em>Lady of Space and Time</em>, the <em>Lion Cub of Cintra</em>, and the <em>Child of the Elder Blood</em>, she can manipulate space and time, travel between worlds, and influence the course of events in ways few can.</p>
        </div>
    </div>

    <h2>Why name a LLM Cirilla</h2>
    <div class="flex-container">
        <div class="text">
            <p>Unlike rulers who inherit authority, <em>Cirilla</em> embodies potential realized through learning, experience, and adaptability. She is resilient, capable of navigating complex and unpredictable worlds, and able to respond to challenges with skill and precision—qualities that mirror how a language model can shift between tasks, domains, and contexts.</p>
            <p>Guided by mentors and shaped by hardships, Ciri develops her abilities quickly, mastering both strategy and instinct while remaining flexible in the face of unforeseen circumstances.</p>
            <p>Her combination of innate talent, adaptability, and the capacity for growth makes her a fitting symbol for a language model designed to acquire knowledge, evolve over time, and connect information across domains.</p>
        </div>
        <div class="img-container">
            <img src="https://github.com/AnthonyP57/Radovid---a-LLM-made-on-a-budget/blob/master/img/Ciri.webp?raw=true" alt="Ciri" width="250">
            <div class="img-caption"><em>Fig.2 Ciri Gwent card by Anna Podedworna</em></div>
        </div>
    </div>

    <h2>What is a LLM</h2>
    <p>On a high level: imagine a toddler with a huge amount of knowledge but still possessing a toddler-like way of reasoning and understanding.</p>
    <p>On a lower level: an LLM is a neural network trained on big data to recognize patterns, generate human-like responses, and predict the most likely next word in a given context. While it can process and recall information efficiently, it lacks true understanding, reasoning, or consciousness, relying only on statistical correlations rather than genuine comprehension. The reasoning of LLMs is being improved in projects (most notably) like DeepSeek, which focus on enhancing the ability to understand context and simulate human-like reasoning.</p>

    <h2>Repo organization</h2>
    <pre><code class="language-bash">Cirilla - a LLM made on a budget/
  │
  ├── BERT/                           # overview of BERT
  │   └── RAG/                        # overview of RAG
  │
  ├── Cirilla_model/                  # implementation of Cirilla LLM
  │   ├── model.py
  │   ...
  │
  ├── Decoder_only_architecture/      # overview of decoder only transformer architecture
  │   └── Llama2/                     # implementation of Llama 2 inference loop
  │   └── Mistral/                    # overview of the Mistral 7B architecture and inference tricks
  │
  ├── LLM_pieces/                     # elements of decoder-only model you can use
  │   ├── SMoE.py                     # Sparse mixture of Experts
  │   ...
  │
  ├── synth_data/
  │   ├── fandom_create_instruct.py
  │   ├── fandom_scraper.py
  │   ├── Ollama_create_instruct.py
  │   ├── reason_gym_synthetic.py
  │   └── rm_duplicate_instruct.py
  │
  ├── Training_optimizations/
  │   ├──FlexAttention/
  │   └── HF_kernels/
  │     └── examples/
  │
  └── Transformer_from_scratch/
      ├── model.py
      ├── dataset.py
      ├── train.py
      └── LongNet.py</code></pre>

    <!-- Documentation Section -->
    <div class="docs-section">
    <h2>Documentation</h2>

        <hr class="doc-divider" />

        <div class="function-doc" id="OllamaCurate">
        <h3>OllamaCurate</h3>
        <div class="code-label">class cirilla.synth_data.OllamaCurate(model, system_prompt, response_template:BaseModel)</div>

        <!-- <div class="function-signature">
            OllamaCurate(model, system_prompt, response_template:BaseModel)
        </div> -->

            <div class="indented">
                <p>Generall class for creating syntetic datasets with ollama</p>

                <table style="margin-bottom: 50px;">
                <tr><th>Argument</th><th>Type</th><th>Description</th></tr>
                <tr><td><code>model</code></td><td>str</td><td>name of ollama model e.g. <code>"llama3.1:8b"</code></td></tr>
                <tr><td><code>system_prompt</code></td><td>str</td><td>system prompt for the chosen ollama model, used by its functions: <code>__call__</code>,  <code>dynamic_hierarchical_summary</code> and <code>single_pass_summary</code></td></tr>
                <tr><td><code>response_template</code></td><td>pydantic.BaseModel</td><td>template for structured responses, used by its functions: <code>__call__</code>,  <code>dynamic_hierarchical_summary</code>,  <code>single_pass_summary</code> and <code>multi_turn</code></td></tr>
                </table>
                
                <div class="code-label">__call__(paths, save_to, seed, checkpoint, skip) → None</div>
        
                <div class="indented">
                <p>Create synthetic instructions (question-answer pairs)<br>to see how to turn the instructions into a <code>.jsonl</code> file see the <a href="#rm_duplicate_instructs">rm_duplicate_instructs</a> function</p>
                
                        <table>
                        <tr><th>Argument</th><th>Type</th><th>Description</th></tr>
                        <tr><td><code>paths</code></td><td>list[Path]</td><td>paths to .txt files containing texts to create question answer pairs</td></tr>
                        <tr><td><code>save_to</code></td><td>Path</td><td>folder to save generated question answer pairs to</td></tr>
                        <tr><td><code>seed</code></td><td>int</td><td>seed for the ollama model</td></tr>
                        <tr><td><code>checkpoint</code></td><td>int</td><td>save the generated question answer pairs to the <code>save_to folder</code> after <code>checkpoint</code> iterations</td></tr>
                        <tr><td><code>skip</code></td><td>bool</td><td>if <code>True</code>, skip the already existing question answer pairs, else save as <code>existing_name_{i}</code> where <code>i</code> is the number of already existing files of the same name</td></tr>
                        </table>

                    <p><strong>Example:</strong></p>
                    <pre><code class="language-python"><pre><code class="language-python">import os
import random

class Response(BaseModel):
    question: str = Field(description="What question is appropriate to this text?")
    answer: str = Field(description="Answer to the question")

sys_prompt = """
You are an expert dataset annotator for instruction-tuning large language models. 
Your task is to create high-quality question-answer pairs from provided texts 
for training instruct models.

Guidelines:
- Keep the question relevant and informative for learners.
- Avoid using markdown or any unnecessary formatting.
- You can ask to elaborate based on a keyword or phrase in the text.
- You can ask about the plot if the text is a story.
- Do not use overly formal language.
- Use only the information provided in the text.
- If the text states that any part of it is from Netflix, or mentions that a section is from Netflix,
  ignore that part and do not include it in the question or answer.
- If user specifies already created question and answer pair, find a different question and answer
  pair that is different from the one provided. If this is impossible use different words then the ones
  provided.
- Return the output strictly as a JSON with two fields: "question" and "answer".
"""

folder = "./witcher_fandom"
paths = os.listdir(folder)
paths = [os.path.join(folder, p) for p in paths]
print(f"{len(paths)} paths found")

for _ in range(3):
        dual = OllamaCurate(
            model="qwen3:8b",
            system_prompt=sys_prompt,
            response_template=Response
        )
        dual(
            paths,
            save_to="./witcher_synthetic_instruct/qwen3:8b",
            seed=random.randint(0, 1000),
            checkpoint=10
            skip=False,
        )</code></pre></code></pre>

                <p><strong>Example created output file: <code class="language-bash">./witcher_synthetic_instruct/qwen3:8b/Deadly Plot.json</code></strong></p>
<pre><code class="language-json">{
    "question": "What location must the player meet Dijkstra at for the A Deadly Plot quest?",
    "answer": "Passiflora."
}
</code></pre>
                <p>The output file's contents depend on whatever was present in the corresponding text file:<br><code class="language-bash">./witcher_fandom/Deadly Plot.txt</code></p>

                </div>

                <div class="code-label" style="margin-top: 50px;">single_pass_summary(paths, save_to, seed, num_predict, use_response_template) → None</div>
        
                <div class="indented">
                <p>For provided <code>.txt</code> files create a simple summary of them<br>See how to turn summaries into a dataset: <a href="#gather_summaries">gather_summaries</a> <a href="#summaries_to_instruct">summaries_to_instruct</a></p>
                
                        <table>
                        <tr><th>Argument</th><th>Type</th><th>Description</th></tr>
                        <tr><td><code>paths</code></td><td>list[Path]</td><td>paths to <code>.txt</code> files containing texts to summarize</td></tr>
                        <tr><td><code>save_to</code></td><td>Path</td><td>folder to save generated summaries to</td></tr>
                        <tr><td><code>seed</code></td><td>int</td><td>seed for the ollama model</td></tr>
                        <tr><td><code>num_predict</code></td><td>int</td><td>maximum number of tokens the ollama model can generate</td></tr>
                        <tr><td><code>use_response_template</code></td><td>bool</td><td>whether to use a response template, that is set when initialing the <code>OllamaCurate</code> class</td></tr>
                        </table>

                    <p><strong>Example:</strong></p>
                    <pre><code class="language-python"><pre><code class="language-python">import os
import random
from pydantic import BaseModel, Field

class Response(BaseModel):
    summary: str = Field(description="Final summary of the entire text. Pure summary only,
    no introduction or reasoning.")

paths = os.listdir("./witcher_fandom")
paths = [os.path.join("./witcher_fandom", p) for p in paths]

print(f"{len(paths)} paths found")

for m in ["granite3.1-moe:3b"]:
    model = OllamaCurate(model=m, system_prompt="", response_template=Response)
    model.single_pass_summary(
        paths,
        save_to=f"./witcher_synth_summaries/{m}",
        seed=random.randint(0, 1000),
        num_predict=4096,
        use_response_template=True,
    )</code></pre></code></pre>

            </div>

                            <div class="code-label" style="margin-top: 50px;">dynamic_hierarchical_summary(paths, save_to, chunk_lines, seed, num_predict, max_words_summary, use_response_template) → None</div>
        
                <div class="indented">
                <p>For provided <code>.txt</code> files create a hierarchical summary of them. Meaning that for a text divided into chunks we subsequently generate a summary for each chunk, and then generate a final summary based on the summaries of the chunks</p>
                
                        <table>
                        <tr><th>Argument</th><th>Type</th><th>Description</th></tr>
                        <tr><td><code>paths</code></td><td>list[Path]</td><td>paths to <code>.txt</code> files containing texts to summarize</td></tr>
                        <tr><td><code>save_to</code></td><td>Path</td><td>folder to save generated summaries to</td></tr>
                        <tr><td><code>chunk_lines</code></td><td>int</td><td>the number of lines by which to chunk the text e.g. for a text of 1000 lines and <code>chunk_lines=200</code> the text will be divided into 5 chunks of 200 lines each, all the chunks will get their own summary and based on them a final summary will be generated</td></tr>
                        <tr><td><code>seed</code></td><td>int</td><td>seed for the ollama model</td></tr>
                        <tr><td><code>num_predict</code></td><td>int</td><td>maximum number of tokens the ollama model can generate</td></tr>
                        <tr><td><code>max_words_summary</code></td><td>int</td><td>maximum number of words ollama should target for the summary</td></tr>
                        <tr><td><code>use_response_template</code></td><td>bool</td><td>whether to use a response template, that is set when initialing the <code>OllamaCurate</code> class</td></tr>
                        </table>

                    <p><strong>Example:</strong></p>
                    <pre><code class="language-python"><pre><code class="language-python">import os
from pydantic import BaseModel, Field
import os
import random

class Response(BaseModel):
    summary: str = Field(description="Summary of the text, without the thinking process and without any
    introduction. Provide only pure summary, be expressive but stick to the maximum number of words
    that were provided.")

paths = os.listdir('./witcher_texts')
paths = [os.path.join('./witcher_texts', p) for p in paths]

print(f"{len(paths)} paths found")

for m in ['llama3.2:3b', 'llama3.1:8b', 'qwen3:8b']:

    model = OllamaCurate(model=m,
                        system_prompt="",
                        response_template=Response
                        )
    
    model.dynamic_hierarchical_summary(paths,
                                    save_to=f'./synth_sumarries/witcher_texts/{m}',
                                    chunk_lines=100,
                                    seed=random.randint(0, 1000),
                                    num_predict=2048,
                                    max_words_summary=500,
                                    use_response_template=True
                                    )</code></pre></code></pre>

            </div>

            <div class="code-label" style="margin-top: 50px;">multi_turn(paths, save_to, bar, n_turns_range, seed, prob_chance_new_context) → None</div>
        
                <div class="indented">
                <p>Create synthetic multi turn instructions (question-answer pairs)</p>
                
                        <table>
                        <tr><th>Argument</th><th>Type</th><th>Description</th></tr>
                        <tr><td><code>paths</code></td><td>list[Path]</td><td>paths to .txt files containing texts to create multi turn question answer pairs</td></tr>
                        <tr><td><code>save_to</code></td><td>Path</td><td>folder to save generated mult turn question answer pairs to</td></tr>
                        <tr><td><code>bar</code></td><td>tqdm.tqdm</td><td>tqdm bar to track progress</td></tr>
                        <tr><td><code>n_turns_range</code></td><td>tuple[int,int]</td><td>range of number of turns to generate, e.g. <code>(2, 4)</code> will generate question answer pairs with 2 to 4 turns (the actual number of turns will be randomly chosen from this range and consistent for all the files)</td></tr>
                        <tr><td><code>seed</code></td><td>int</td><td>seed for the ollama model</td></tr>
                        <tr><td><code>prob_chance_new_context</code></td><td>float</td><td>Probability of starting a new context for the question answer pairs e.g. with a given chance we can continue the multi turn conversation with a new context from another randomly chosen <code>.txt</code> file</td></tr>
                        </table>

                    <p><strong>Example:</strong></p>
                    <pre><code class="language-python"><pre><code class="language-python">from pydantic import BaseModel, Field
import os
import random

class Response(BaseModel):
    question: str = Field(description="What question is appropriate to this text?")
    answer: str = Field(description="Answer to the question")

paths = os.listdir('./witcher_fandom')
paths = [os.path.join('./witcher_fandom', p) for p in paths]

bar = tqdm(total=3*3*len(paths))
for model in ['qwen3:8b', 'phi4', 'llama3.1:8b']:

    for _ in range(3):
        ol = OllamaCurate(model, "", Response)
        ol.multi_turn(paths,
        save_to=f'./synth_multi_round/{model}',
        bar=bar,
        n_turns_range=(2, 5),
        seed=random.randint(0, 1000),
        prob_chance_new_context=0.3)</code></pre></code></pre>

            </div>

        </div>

    </div>

        <div class="function-doc" id="rm_duplicate_instructs">
        <h3>rm_duplicate_instructs</h3>
        <div class="code-label">func cirilla.synth_data.rm_duplicate_instructs(main_dir, save_to) → None</div>

            <div class="indented">
                <p>remove duplicate synthetic instructions</p>

                <table style="margin-bottom: 50px;">
                <tr><th>Argument</th><th>Type</th><th>Description</th></tr>
                <tr><td><code>main_dir</code></td><td>Path</td><td>path to the directory containing the synthetic instructions</td></tr>
                <tr><td><code>out_path</code></td><td>Path</td><td>path to save the cleaned instructions to</td></tr>
                </table>
                
                <p><strong>Example:</strong></p>
                <pre><code class="language-python"><pre><code class="language-python">main_dir = './witcher_synthetic_instruct'
save_to = './witcher_synthetic_instruct.jsonl'

rm_duplicate_instructs(main_dir, save_to)</code></pre></code></pre>

<p><strong>Example input file: <code class="language-bash">./witcher_synthetic_instruct/qwen3:8b/Deadly Plot.json</code></strong></p>
<pre><code class="language-json">{
    "question": "What is the objective of the A Deadly Plot quest in The Witcher 3: Wild Hunt?",
    "answer": "The objective of the A Deadly Plot quest is to help Dijkstra ..."
}</code></pre>

<p><strong>Example input file: <code class="language-bash">./witcher_synthetic_instruct/qwen3:8b/Deadly Plot_1.json</code></strong></p>
<pre><code class="language-json">{
    "question": "What is the main objective of the A Deadly Plot quest in The Witcher 3: Wild Hunt?",
    "answer": "The objective of the A Deadly Plot quest is to help Dijkstra ..."
}</code></pre>

Since the second instruction is nearly identical, it will be deleted in the final <code>.jsonl</code> file.

<p><strong>Example created output file (obviously the actual <code>.jsonl</code> file is saved in single lines): <code class="language-bash">./witcher_synthetic_instruct.jsonl</code></strong></p>
<pre><code class="language-json">{"subject": "Silver sword", "text": [
    {"role": "user", "content": "What material are silver swords made from in The Witcher?"},
    {"role": "assistant", "content": "Meteoric iron coated with silver and inscribed with magical runes"}
    ], "data type": "conv", "model": "llama3.1:8b"}
...</code></pre>
            </div>
        </div>

        <div class="function-doc" id="gather_summaries">
        <h3>gather_summaries</h3>
        <div class="code-label">func cirilla.synth_data.gather_summaries(in_path, out_path) → None</div>

            <div class="indented">
                <p>turn summaries into a training dataset</p>

                <table style="margin-bottom: 50px;">
                <tr><th>Argument</th><th>Type</th><th>Description</th></tr>
                <tr><td><code>in_path</code></td><td>Path</td><td>path to the directory containing summaries in <code>.txt</code> files, they can be nested</td></tr>
                <tr><td><code>out_path</code></td><td>Path</td><td>path to save the instructions to</td></tr>
                </table>
                
                <p><strong>Example:</strong></p>
                <pre><code class="language-python"><pre><code class="language-python">in_path = './witcher_synth_summaries'
out_path = './witcher_summaries_gathered.jsonl'

gather_summaries(in_path, out_path)</code></pre></code></pre>

<p><strong>Example input file: <code class="language-bash">./witcher_synth_summaries/llama3.1:8b/Alchemist.txt</code></strong></p>
<pre><code class="language-python">In the Witcher lore, Alchemists are individuals who practice alchemy ...</code></pre>

<p><strong>Example created output file (obviously the actual <code>.jsonl</code> file is saved in single lines): <code class="language-bash">./witcher_summaries_gathered.jsonl</code></strong></p>
<pre><code class="language-json">{
    "subject": "sorcerers",
    "text":"Mages, or sorcerers/sorceresses, ...",
    "data type": "plain text", "source": "fandom", "model": "llama3.2:3b"
}
...</code></pre>
            </div>
        </div>

        <div class="function-doc" id="summaries_to_instruct">
        <h3>summaries_to_instruct</h3>
        <div class="code-label">func cirilla.synth_data.summaries_to_instruct(in_path, out_path) → None</div>

            <div class="indented">
                <p>turn summaries into simple instructions. The user questions are chosen as one from a list</p>

                <table style="margin-bottom: 50px;">
                <tr><th>Argument</th><th>Type</th><th>Description</th></tr>
                <tr><td><code>in_path</code></td><td>Path</td><td>path to the directory containing summaries in <code>.txt</code> files, they can be nested</td></tr>
                <tr><td><code>out_path</code></td><td>Path</td><td>path to save the instructions to</td></tr>
                </table>
                
                <p><strong>Example:</strong></p>
                <pre><code class="language-python"><pre><code class="language-python">in_path = './witcher_synth_summaries'
out_path = './witcher_summaries_gathered_instr.jsonl'

gather_summaries(in_path, out_path)</code></pre></code></pre>

<p><strong>Example input file: <code class="language-bash">./witcher_synth_summaries/llama3.1:8b/Alchemist.txt</code></strong></p>
<pre><code class="language-python">In the Witcher lore, Alchemists are individuals who practice alchemy ...</code></pre>

<p><strong>Example created output file (obviously the actual <code>.jsonl</code> file is saved in single lines): <code class="language-bash">./witcher_summaries_gathered_instr.jsonl</code></strong></p>
<pre><code class="language-json">{
    "subject": "Ravik",
    "text": [
    {"role": "user", "content": "What is notable about Ravik?"},
    {"role": "assistant", "content": "Ravik, also known as Ravvy, was a friend ..."}
    ],
    "data type": "plain text", "source": "fandom", "model": "llama3.2:3b"
}
...</code></pre>
As you may notice the user's question is asking about Ravik, which is the name of the file <code>./.../Ravik.txt</code>
            </div>
        </div>

        <div class="function-doc" id="get_synth_reasoning_dataset">
        <h3>get_synth_reasoning_dataset</h3>
        <div class="code-label">func cirilla.synth_data.get_synth_reasoning_dataset(out_path, n_samples, specs) → None</div>

            <div class="indented">
                <p>create synthetic reasoning dataset with <a href="https://github.com/open-thought/reasoning-gym">reasoning_gym</a></p>

                <table style="margin-bottom: 50px;">
                <tr><th>Argument</th><th>Type</th><th>Description</th></tr>
                <tr><td><code>out_path</code></td><td>Path</td><td>path to save the synthetic reasoning dataset to</td></tr>
                <tr><td><code>n_samples</code></td><td>int</td><td>How many samples of the synthetic reasoning dataset to create (each sample contains 100 data points)</td></tr>
                <tr><td><code>specs</code></td><td>list[reasoning_gym.composite.DataSpec]</td><td>specs for creating the synthetic reasoning dataset</td></tr>
                </table>
                
                <p><strong>Example:</strong></p>
                <pre><code class="language-python"><pre><code class="language-python">out_path = './reason_gym_synth.jsonl'
n_samples = 400 # will contain 40'000 data points

get_synth_resoning_dataset(out_path, n_samples)</code></pre></code></pre>

<p><strong>Example output file: <code class="language-bash">./reason_gym_synth.jsonl</code> (obviously the actual <code>.jsonl</code> file is saved in single lines)</strong></p>
<pre><code class="language-json">{"subject": "simple_equations", "text": [
    {"role": "user", "text": "Solve for g: 65*g = 3185"},
    {"role": "assistant", "text": "49"}],
    "data type": "conv"}
{"subject": "needle_haystack", "text": [
    {"role": "user", "text": "Ismaeel worships lions. Yann embraces travel blogging. Malakhy
    scorns historical documentaries. Kabeer cherishes sketching. \nWho scorns historical
    documentaries? Reply only with a name."},
    {"role": "assistant", "text": "Malakhy"}],
    "data type": "conv"}
...</code></pre>
            </div>
        </div>

        <div class="function-doc" id="vllm_multi_turn">
        <h3>vllm_multi_turn</h3>
        <div class="code-label">func cirilla.synth_data.vllm_multi_turn(paths, save_to, batch_size, system_prompt, n_turns, template, model, prob_chance_new_context) → None</div>

            <div class="indented">
                <p>create synthetic multi turn conversations about given topics with <a href="https://github.com/vllm-project/vllm">vllm</a><br>to see how to turn the instructions into a <code>.jsonl</code> file see the <a href="#multi_turn_gather">multi_turn_gather</a> function</p>

                <table style="margin-bottom: 50px;">
                <tr><th>Argument</th><th>Type</th><th>Description</th></tr>
                <tr><td><code>paths</code></td><td>list[Path]</td><td>list of paths to the contexts for the conversations</td></tr>
                <tr><td><code>save_to</code></td><td>Path</td><td>path to save the synthetic multi turn dataset to</td></tr>
                <tr><td><code>batch_size</code></td><td>int</td><td>batch size for the conversations</td></tr>
                <tr><td><code>system_prompt</code></td><td>str</td><td>system prompt for the conversations</td></tr>
                <tr><td><code>n_turns</code></td><td>int</td><td>number of turns for the conversations, it usually means the maximum number of turns, since some of the turns may fail and will thus be empty</td></tr>
                <tr><td><code>template</code></td><td>pydantic.BaseModel</td><td>template for the conversations</td></tr>
                <tr><td><code>model</code></td><td>str</td><td>model form <a href="https://huggingface.co/models">huggingface</a> for the conversations</td></tr>
                <tr><td><code>prob_chance_new_context</code></td><td>float</td><td>probability of a new context being added into the conversations</td></tr>
                </table>
                
                <p><strong>Example:</strong></p>
                <pre><code class="language-python"><pre><code class="language-python">paths_ = ['./summaries/granite3.1-moe:3b',
            './summaries/llama3.1:8b',
            './summaries/llama3.2:3b',
            './summaries/qwen3:8b']

paths = [[os.path.join(p, f) for f in os.listdir(p)] for p in paths_]

for model in ["unsloth/granite-3.2-2b-instruct-unsloth-bnb-4bit"]:
    for i, sub_paths in enumerate(paths):
        for _ in range(1):
        
            vllm_multi_turn(sub_paths,
            save_to=f'./synth_multi_round/{model.split("/")[1]}/{paths_[i].split("/")[-1]}',
            model=model)</code></pre></code></pre>
        <p>Where the summaries may come from <a href="#OllamaCurate">OllamaCurate.single_pass_summary</a></p>

        <p><strong>Example created output file <code class="language-bash">./synth_multi_round/granite-3.2-2b-.../granite3.1-moe:3b/A Book of Tales.json</code>:</strong></p>
<pre><code class="language-json">[
  {
    "question": "Which specific adventure or mystery introduced in The Book of Tales ... ?",
    "answer": "The given text does not specify a single adventure where the ...",
    "context": "A Book of Tales"
  },
  {
    "question": "Which three new playable races \u2013 Gnomes, ... ?",
    "answer": "Gnomes are native to the mountainous region of Mount Carbon in Kovir ...",
    "context": "A Book of Tales"
  },
  {
    "question": "What central role does Radko hold in The Witcher ... ?",
    "answer": "Radko, a soldier under the Bloody Baron's command ...",
    "context": "Radko"
  }
]</code></pre>
            </div>
        </div>

        <div class="function-doc" id="multi_turn_gather">
        <h3>multi_turn_gather</h3>
        <div class="code-label">func cirilla.synth_data.multi_turn_gather(input_path, save_to) → None</div>

            <div class="indented">
                <p>gather multi turn conversations into a <code>.jsonl</code> file</p>

                <table style="margin-bottom: 50px;">
                <tr><th>Argument</th><th>Type</th><th>Description</th></tr>
                <tr><td><code>input_path</code></td><td>Path</td><td>path to a folder with <code>.jsonl</code> files containing multi turn conversations, they can be nested</td></tr>
                <tr><td><code>save_to</code></td><td>Path</td><td>path to save the gathered conversations to</td></tr>
                </table>
                
                <p><strong>Example:</strong></p>
                <pre><code class="language-python"><pre><code class="language-python">inp = './synth_multi_round'
outp = './multi_round.jsonl'

multi_turn_gather(inp, outp)</code></pre></code></pre>
        <p><strong>Example created output file <code class="language-bash">./multi_round.jsonl</code> (obviously the actual <code>.jsonl</code> file is saved in single lines):</strong></p>
<pre><code class="language-json">{
    "subject": "A Little Sacrifice", "text":
    [
    {"role": "user", "content": "What does Sh'eenaz do to help resolve the ... ?"},
    {"role": "assistant", "content": "Sh'eenaz agrees to give up her tail for ..."},
    {"role": "user", "content": "What does Sh'eenaz do to ... ?"},
    {"role": "assistant", "content": "Sh'eenaz ..."},
    {"role": "user", "content": "What is the nature of Geralt's ... ?"},
    {"role": "assistant", "content": "Geralt ..."}
    ],
    "data type": "conv", "source": "fandom",
    "metadata": {"contexts": ["A Little Sacrifice", "A Little Sacrifice", "A Little Sacrifice"]}
}
</code></pre>
            </div>
        </div>

        <div class="function-doc" id="scrape_fandom">
        <h3>scrape_fandom</h3>
        <div class="code-label">func fandom_scraper.scrape_fandom(in_path, out_path, instruct_path, n_workers, wiki, lang) → None</div>

            <div class="indented">
                <p>scrape a given fandom wiki. Use huggingface's span maker (Named Entity Recognition) to search for new pages to scrape<br><br><a href=https://github.com/AnthonyP57/fandom_scraper>fandom_scraper</a> is a standalone package</p>

                <table style="margin-bottom: 50px;">
                <tr><th>Argument</th><th>Type</th><th>Description</th></tr>
                <tr><td><code>in_path</code></td><td>Path</td><td>path to a folder with <code>.json</code> files containing lists with key words to search for first (so-called seeds)</td></tr>
                <tr><td><code>out_path</code></td><td>Path</td><td>path to save the scraped texts to</td></tr>
                <tr><td><code>instruct_path</code></td><td>Path</td><td>path to save the scraped instructions to</td></tr>
                <tr><td><code>n_workers</code></td><td>int</td><td>how many async wokers to use to fetch the fandom pages</td></tr>
                <tr><td><code>wiki</code></td><td>str</td><td>what fandom wiki to scrape</td></tr>
                <tr><td><code>lang</code></td><td>str</td><td>what language to use for the fandom</td></tr>
                </table>
                
                <p><strong>Example:</strong></p>
                <pre><code class="language-python"><pre><code class="language-python">in_path = "./witcher_json"
out_path = "./async_fandom"
instruct_path = "./async_fandom_instruct"

scrape_fandom(in_path,
            out_path,
            instruct_path,
            n_workers = 50,
            wiki = "Witcher",
            lang = "en"
            )</code></pre></code></pre>

<p><strong>Example input file: <code class="language-bash">./witcher_json/witcher_1.json</code></strong></p>
<pre><code class="language-json">[
    "Geralt of Rivia", "Triss Merigold", "Vesemir", "Leo", "Lambert", 
    "Eskel", "Alvin", "Shani", "Zoltan Chivay", "Dandelion (Jaskier)", 
    "King Foltest", "Adda the White", ...
]
</code></pre>

<p><strong>Example created output file: <code class="language-bash">./async_fandom/Geralt of Rivia.txt</code></strong></p>
<pre><code class="language-json">Geralt of Rivia
Sub-Pages:
Main
Biography
Geralt was born as the son of the sorceress Visenna and presumably, the warrior Korin. Shortly after his
birth, his mother left him with the School of the Wolf at the stronghold of Kaer Morhen. There, Geralt
was made and trained to become a Witcher.
As a child, he ...
</code></pre>

<p><strong>Example created output file: <code class="language-bash">./async_fandom_instruct/Geralt of Rivia.json</code></strong></p>
<pre><code class="language-json">{
  "Who is Geralt of Rivia?": "Geralt of Rivia is a witcher and the protagonist from The Witcher ...",
  "What is Geralt's nickname?": "Geralt has a number of nicknames, with the more well known ones ...",
  "Who trained Geralt to become a Witcher?": "Geralt was trained by his mentor Vesemir, who he ...",
  "What is Geralt's profession?": "Geralt is a monster slayer for hire. He travels the world on ...",
  "What is the Trial of The Grasses?": "The Trial of The Grasses was a painful process that ..."
}
</code></pre>

            </div>
        </div>

        <div class="function-doc" id="instructions_into_conv">
        <h3>instructions_into_conv</h3>
        <div class="code-label">func fandom_scraper.instructions_into_conv(input_path, out_path) → None</div>

            <div class="indented">
                <p>convert scraped fandom instructions into a <code>.jsonl</code> file</p>

                <table style="margin-bottom: 50px;">
                <tr><th>Argument</th><th>Type</th><th>Description</th></tr>
                <tr><td><code>input_path</code></td><td>Path</td><td>path to a folder with scraped instructions</td></tr>
                <tr><td><code>out_path</code></td><td>Path</td><td>path to save the <code>.jsonl</code> file to</td></tr>
                </table>
                
                <p><strong>Example:</strong></p>
                <pre><code class="language-python"><pre><code class="language-python">instructions_into_conv(input_path="./async_fandom_instruct",
                       out_path="./async_fandom_instruct_gathered.jsonl")</code></pre></code></pre>

<p><strong>Example input file: <code class="language-bash">./async_fandom_instruct/Geralt of Rivia.json</code></strong></p>
<pre><code class="language-json">{
  "Who is Geralt of Rivia?": "Geralt of Rivia is a witcher and the protagonist from The Witcher ...",
  "What is Geralt's nickname?": "Geralt has a number of nicknames, with the more well known ones ...",
  "Who trained Geralt to become a Witcher?": "Geralt was trained by his mentor Vesemir, who he ...",
  "What is Geralt's profession?": "Geralt is a monster slayer for hire. He travels the world on ...",
  "What is the Trial of The Grasses?": "The Trial of The Grasses was a painful process that ..."
}
</code></pre>

<p><strong>Example created output file (obviously the actual <code>.jsonl</code> file is saved in single lines): <code class="language-bash">./async_fandom_instruct_gathered.jsonl</code></strong></p>
<pre><code class="language-json">{"subject": "Geralt of Rivia", "text": [
    {"role": "user", "content": "Who is Geralt of Rivia?"},
    {"role": "assistant", "content": "Geralt of Rivia is a witcher and the protagonist from ..."}
    ], "data type": "conv", "source": "fandom"}
{"subject": "Geralt of Rivia", "text": [
    {"role": "user", "content": "What is Geralt's nickname?"},
    {"role": "assistant", "content": "Geralt has a number of nicknames, with the more well known ones ..."}
    ], "data type": "conv", "source": "fandom"}
...</code></pre>

            </div>
        </div>

</div>
</main>

<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-python.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-bash.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-json.min.js"></script>
<script>
// Search functionality
const searchInput = document.getElementById('search');
searchInput.addEventListener('input', function() {
    const query = this.value.toLowerCase();
    const functions = document.querySelectorAll('.function-doc');
    functions.forEach(fn => {
        const text = fn.innerText.toLowerCase();
        fn.style.display = text.includes(query) ? '' : 'none';
    });
});
</script>

<script>
// Toggle arrows and show/hide category items
const headers = document.querySelectorAll('.category-header');
headers.forEach(header => {
    header.addEventListener('click', () => {
        const key = header.dataset.toggle + "-items";
        const items = document.getElementById(key);
        const arrow = header.querySelector('.arrow');

        if (items.style.display === "none" || items.style.display === "") {
            items.style.display = "flex";
            arrow.textContent = "▴"; // arrow up
        } else {
            items.style.display = "none";
            arrow.textContent = "▾"; // arrow down
        }
    });
});
</script>

<script>
// Adjust scroll for sidebar links to headers
document.querySelectorAll('.sidebar a[href^="#"]').forEach(link => {
    link.addEventListener('click', function(e) {
        e.preventDefault(); // prevent default jump
        const targetId = this.getAttribute('href').substring(1);
        const target = document.getElementById(targetId);
        if (target) {
            const yOffset = -80; // same as header height
            const y = target.getBoundingClientRect().top + window.pageYOffset + yOffset;
            window.scrollTo({ top: y, behavior: 'smooth' });
        }
    });
});
</script>


</body>
</html>